{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698b5b8e",
   "metadata": {},
   "source": [
    "# Compound Protocol Wallet Risk Scoring System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e612a",
   "metadata": {},
   "source": [
    "### Necessary Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c893bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: web3 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (7.12.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\amiku\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\amiku\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\amiku\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\amiku\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\amiku\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\amiku\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\amiku\\anaconda3\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\amiku\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: eth-abi>=5.0.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-account>=0.13.6 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (0.13.7)\n",
      "Requirement already satisfied: eth-hash>=0.5.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (0.7.1)\n",
      "Requirement already satisfied: eth-typing>=5.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (5.2.1)\n",
      "Requirement already satisfied: eth-utils>=5.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (5.3.0)\n",
      "Requirement already satisfied: hexbytes>=1.2.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (1.3.1)\n",
      "Requirement already satisfied: aiohttp>=3.7.4.post0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (3.12.14)\n",
      "Requirement already satisfied: pydantic>=2.4.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (2.11.7)\n",
      "Requirement already satisfied: pywin32>=223 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (311)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (4.12.2)\n",
      "Requirement already satisfied: types-requests>=2.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (2.32.0.20250306)\n",
      "Requirement already satisfied: websockets<16.0.0,>=10.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (15.0.1)\n",
      "Requirement already satisfied: pyunormalize>=15.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from web3) (16.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (1.18.3)\n",
      "Requirement already satisfied: parsimonious<0.11.0,>=0.10.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-abi>=5.0.1->web3) (0.10.0)\n",
      "Requirement already satisfied: regex>=2022.3.15 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from parsimonious<0.11.0,>=0.10.0->eth-abi>=5.0.1->web3) (2024.9.11)\n",
      "Requirement already satisfied: bitarray>=2.4.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (3.5.1)\n",
      "Requirement already satisfied: eth-keyfile<0.9.0,>=0.7.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (0.8.1)\n",
      "Requirement already satisfied: eth-keys>=0.4.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (0.7.0)\n",
      "Requirement already satisfied: eth-rlp>=2.1.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (2.2.0)\n",
      "Requirement already satisfied: rlp>=1.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (4.1.0)\n",
      "Requirement already satisfied: ckzg>=2.0.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (2.1.1)\n",
      "Requirement already satisfied: pycryptodome<4,>=3.6.6 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-keyfile<0.9.0,>=0.7.0->eth-account>=0.13.6->web3) (3.21.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from eth-utils>=5.0.0->web3) (0.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from pydantic>=2.4.0->web3) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from pydantic>=2.4.0->web3) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from pydantic>=2.4.0->web3) (0.4.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (0.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amiku\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install web3 pandas numpy matplotlib seaborn scikit-learn scipy requests joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68211a14",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2eab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from web3 import Web3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa64010",
   "metadata": {},
   "source": [
    "# Feature selection and risk scoring for Compound protocol transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf349f",
   "metadata": {},
   "source": [
    "## This cell is responsible for loading wallet addresses from the input CSV file (`Wallet id - Sheet1.csv`). It performs the following actions:\n",
    "- Reads the CSV file containing wallet addresses.\n",
    "- Attempts to automatically detect the column containing wallet addresses from a list of common column names (e.g., `wallet_id`, `address`, `userWallet`).\n",
    "- If the column is not automatically detected, it prompts the user to specify the correct column name.\n",
    "- Extracts all unique, non-empty wallet addresses from the selected column and stores them in a list for further analysis.\n",
    "- Prints out the number of wallet addresses loaded and displays a sample for verification.\n",
    "\n",
    "This step ensures that the subsequent risk scoring process uses the correct set of wallet addresses as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7d832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundFeatureAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the feature analyzer for Compound protocol data\n",
    "        \"\"\"\n",
    "        self.feature_correlations = {}\n",
    "        self.risk_indicators = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def engineer_compound_features(self, df):\n",
    "        \"\"\"\n",
    "        Engineer features from Compound transaction data\n",
    "        \n",
    "        Expected columns in df:\n",
    "        - userWallet: wallet address\n",
    "        - action: type of action (supply, redeem, borrow, repay, liquidate)\n",
    "        - amount_usd: USD value of transaction\n",
    "        - asset: asset symbol (ETH, USDC, DAI, etc.)\n",
    "        - timestamp_dt: datetime of transaction\n",
    "        - market: compound market (cToken address or symbol)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        if 'timestamp_dt' not in df.columns and 'timestamp' in df.columns:\n",
    "            df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Group by wallet and compute base features\n",
    "        features_df = df.groupby('userWallet').agg({\n",
    "            'action': ['count', 'nunique'],\n",
    "            'amount_usd': ['sum', 'mean', 'std', 'max', 'min'],\n",
    "            'asset': 'nunique',\n",
    "            'market': 'nunique',\n",
    "            'timestamp_dt': ['min', 'max']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        features_df.columns = ['userWallet', 'total_transactions', 'unique_actions',\n",
    "                              'total_volume_usd', 'avg_transaction_usd', 'transaction_volatility',\n",
    "                              'max_transaction_usd', 'min_transaction_usd',\n",
    "                              'unique_assets', 'unique_markets',\n",
    "                              'first_activity', 'last_activity']\n",
    "        \n",
    "        # Action-specific counts\n",
    "        action_counts = df.groupby(['userWallet', 'action']).size().unstack(fill_value=0)\n",
    "        action_columns = ['supply_count', 'redeem_count', 'borrow_count', 'repay_count', 'liquidate_count']\n",
    "        \n",
    "        for col in action_columns:\n",
    "            action_name = col.replace('_count', '')\n",
    "            if action_name in action_counts.columns:\n",
    "                features_df[col] = features_df['userWallet'].map(action_counts[action_name]).fillna(0)\n",
    "            else:\n",
    "                features_df[col] = 0\n",
    "                \n",
    "        # Derived temporal features\n",
    "        features_df['activity_days'] = (features_df['last_activity'] - features_df['first_activity']).dt.days + 1\n",
    "        features_df['avg_days_between_tx'] = features_df['activity_days'] / features_df['total_transactions']\n",
    "        features_df['transactions_per_day'] = features_df['total_transactions'] / features_df['activity_days']\n",
    "        \n",
    "        # Behavioral ratios (key risk indicators)\n",
    "        features_df['supply_ratio'] = features_df['supply_count'] / features_df['total_transactions']\n",
    "        features_df['borrow_ratio'] = features_df['borrow_count'] / features_df['total_transactions']\n",
    "        features_df['repay_ratio'] = features_df['repay_count'] / features_df['total_transactions']\n",
    "        features_df['liquidate_ratio'] = features_df['liquidate_count'] / features_df['total_transactions']\n",
    "        features_df['redeem_ratio'] = features_df['redeem_count'] / features_df['total_transactions']\n",
    "        \n",
    "        # Risk-specific derived features\n",
    "        features_df['leverage_indicator'] = features_df['borrow_count'] / (features_df['supply_count'] + 1)\n",
    "        features_df['repayment_discipline'] = features_df['repay_count'] / (features_df['borrow_count'] + 1)\n",
    "        features_df['portfolio_diversity'] = (features_df['unique_assets'] * features_df['unique_markets']) / features_df['total_transactions']\n",
    "        features_df['transaction_size_consistency'] = 1 / (1 + features_df['transaction_volatility'] / (features_df['avg_transaction_usd'] + 1))\n",
    "        \n",
    "        # Market timing features\n",
    "        current_date = pd.Timestamp.now()\n",
    "        features_df['days_since_last_activity'] = (current_date - features_df['last_activity']).dt.days\n",
    "        features_df['recency_score'] = 1 / (1 + features_df['days_since_last_activity'] / 30)  # 30-day decay\n",
    "        \n",
    "        # Fill NaN values\n",
    "        features_df = features_df.fillna(0)\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def calculate_feature_correlations(self, features_df, target_column='liquidate_ratio'):\n",
    "        \"\"\"\n",
    "        Calculate correlations between features and risk indicators\n",
    "        \"\"\"\n",
    "        numeric_columns = features_df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_columns = [col for col in numeric_columns if col != 'userWallet']\n",
    "        \n",
    "        correlations = {}\n",
    "        \n",
    "        for feature in numeric_columns:\n",
    "            if feature != target_column and features_df[feature].var() > 0:\n",
    "                # Pearson correlation\n",
    "                pearson_corr, pearson_p = pearsonr(features_df[feature], features_df[target_column])\n",
    "                \n",
    "                # Spearman correlation (for non-linear relationships)\n",
    "                spearman_corr, spearman_p = spearmanr(features_df[feature], features_df[target_column])\n",
    "                \n",
    "                correlations[feature] = {\n",
    "                    'pearson_corr': pearson_corr,\n",
    "                    'pearson_p_value': pearson_p,\n",
    "                    'spearman_corr': spearman_corr,\n",
    "                    'spearman_p_value': spearman_p,\n",
    "                    'abs_pearson': abs(pearson_corr),\n",
    "                    'abs_spearman': abs(spearman_corr)\n",
    "                }\n",
    "        \n",
    "        return pd.DataFrame(correlations).T\n",
    "    \n",
    "    def create_correlation_heatmap(self, features_df, figsize=(15, 12)):\n",
    "        \"\"\"\n",
    "        Create comprehensive correlation heatmap\n",
    "        \"\"\"\n",
    "        # Select numeric features only\n",
    "        numeric_features = features_df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Remove identifier columns\n",
    "        feature_cols = [col for col in numeric_features.columns \n",
    "                       if col not in ['userWallet'] and numeric_features[col].var() > 0]\n",
    "        \n",
    "        correlation_matrix = numeric_features[feature_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create heatmap\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, \n",
    "                   mask=mask,\n",
    "                   annot=True, \n",
    "                   cmap='RdBu_r', \n",
    "                   center=0,\n",
    "                   square=True,\n",
    "                   linewidths=0.5,\n",
    "                   cbar_kws={\"shrink\": .8},\n",
    "                   fmt='.2f')\n",
    "        \n",
    "        plt.title('Feature Correlation Heatmap - Compound Protocol Risk Factors', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return correlation_matrix\n",
    "    \n",
    "    def justify_feature_weights(self, features_df):\n",
    "        \"\"\"\n",
    "        Create data-driven justification for feature weights with better error handling\n",
    "        \"\"\"\n",
    "        # Check if we have enough data\n",
    "        if len(features_df) < 5:\n",
    "            print(\"Warning: Insufficient data for correlation analysis, using default importance\")\n",
    "            return {}\n",
    "            \n",
    "        # Calculate multiple risk correlations\n",
    "        risk_indicators = ['liquidate_ratio', 'leverage_indicator', 'transaction_volatility', \n",
    "                          'days_since_last_activity', 'borrow_ratio']\n",
    "        \n",
    "        feature_importance = {}\n",
    "        \n",
    "        for risk in risk_indicators:\n",
    "            if risk in features_df.columns and features_df[risk].var() > 0:\n",
    "                try:\n",
    "                    corr_df = self.calculate_feature_correlations(features_df, risk)\n",
    "                    \n",
    "                    if not corr_df.empty:\n",
    "                        # Weight by correlation strength and statistical significance\n",
    "                        for feature, row in corr_df.iterrows():\n",
    "                            if feature not in feature_importance:\n",
    "                                feature_importance[feature] = 0\n",
    "                            \n",
    "                            # Check for valid correlation values\n",
    "                            pearson_corr = row.get('abs_pearson', 0)\n",
    "                            p_value = row.get('pearson_p_value', 1)\n",
    "                            \n",
    "                            if not (np.isnan(pearson_corr) or np.isnan(p_value)):\n",
    "                                # Add weighted importance based on correlation and significance\n",
    "                                significance_weight = 1 if p_value < 0.05 else 0.5\n",
    "                                feature_importance[feature] += pearson_corr * significance_weight\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating correlations for {risk}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Remove NaN values and normalize importance scores\n",
    "        valid_importance = {k: v for k, v in feature_importance.items() \n",
    "                           if not (np.isnan(v) or np.isinf(v))}\n",
    "        \n",
    "        if not valid_importance:\n",
    "            print(\"Warning: No valid feature importance scores calculated\")\n",
    "            return {}\n",
    "            \n",
    "        total_importance = sum(valid_importance.values())\n",
    "        if total_importance > 0:\n",
    "            for feature in valid_importance:\n",
    "                valid_importance[feature] /= total_importance\n",
    "        \n",
    "        return valid_importance\n",
    "    \n",
    "    def create_feature_importance_plot(self, feature_importance, top_n=15):\n",
    "        \"\"\"\n",
    "        Plot feature importance based on correlations\n",
    "        \"\"\"\n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_features = sorted_features[:top_n]\n",
    "        \n",
    "        features, importance = zip(*top_features)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.barh(range(len(features)), importance)\n",
    "        \n",
    "        # Color bars by importance level\n",
    "        colors = ['darkred' if imp > 0.15 else 'red' if imp > 0.10 else 'orange' if imp > 0.05 else 'lightblue' \n",
    "                 for imp in importance]\n",
    "        \n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "        \n",
    "        plt.yticks(range(len(features)), features)\n",
    "        plt.xlabel('Feature Importance Score (Correlation-Based)')\n",
    "        plt.title('Data-Driven Feature Importance for Compound Risk Scoring', fontweight='bold')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add importance threshold lines\n",
    "        plt.axvline(x=0.15, color='red', linestyle='--', alpha=0.7, label='High Importance (>0.15)')\n",
    "        plt.axvline(x=0.10, color='orange', linestyle='--', alpha=0.7, label='Medium Importance (>0.10)')\n",
    "        plt.axvline(x=0.05, color='yellow', linestyle='--', alpha=0.7, label='Low Importance (>0.05)')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return sorted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb621133",
   "metadata": {},
   "source": [
    "## This cell executes the main risk scoring workflow for all wallet addresses provided in the input CSV file. It performs the following steps:\n",
    "- Loads wallet addresses from the specified CSV file.\n",
    "- Fetches transaction data for each wallet from the Etherscan API, focusing on Compound protocol interactions.\n",
    "- Engineers relevant features for each wallet, such as transaction types, volumes, activity patterns, and behavioral ratios.\n",
    "- Calculates a risk score (0-1000) for each wallet based on data-driven weights for different risk categories (liquidity, activity, volatility, diversification, behavioral).\n",
    "- Saves the results to `enhanced_wallet_scores.csv` and the model artifacts (scaler and weights) to `compound_risk_model.joblib`.\n",
    "\n",
    "This process enables you to quantitatively assess the risk profile of each wallet, making it easier to identify high-risk or anomalous behavior in the Compound protocol ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866b3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundRiskScorer:\n",
    "    def __init__(self, infura_url=None, etherscan_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the risk scorer with API credentials and feature analyzer\n",
    "        \"\"\"\n",
    "        self.infura_url = infura_url or \"https://mainnet.infura.io/v3/YOUR_PROJECT_ID\"\n",
    "        self.etherscan_api_key = etherscan_api_key or \"YOUR_ETHERSCAN_API_KEY\"\n",
    "        self.analyzer = CompoundFeatureAnalyzer()\n",
    "        \n",
    "        # Compound V2 and V3 contract addresses\n",
    "        self.compound_contracts = {\n",
    "            'cDAI': '0x5d3a536e4d6dbd6114cc1ead35777bab948e3643',\n",
    "            'cUSDC': '0x39aa39c021dfbae8fac545936693ac917d5e7563',\n",
    "            'cUSDT': '0xf650c3d88d12db855b8bf7d11be6c55a4e07dcc9',\n",
    "            'cETH': '0x4ddc2d193948926d02f9b1fe9e1daa0718270ed5',\n",
    "            'cWBTC': '0xc11b1268c1a384e55c48c2391d8d480264a3a7f4',\n",
    "            'cUSDCv3': '0xc3d688b66703497daa19211eedff47f25384cdc3',\n",
    "            'comptroller': '0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b'\n",
    "        }\n",
    "        \n",
    "        # Data-driven weights (will be calculated from actual data)\n",
    "        self.weights = {}\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def get_transactions(self, wallet_address, start_block=0, max_retries=3):\n",
    "        \"\"\"\n",
    "        Fetch transactions from Etherscan API for a given wallet with retry logic\n",
    "        \"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                url = f\"https://api.etherscan.io/api\"\n",
    "                params = {\n",
    "                    'module': 'account',\n",
    "                    'action': 'txlist',\n",
    "                    'address': wallet_address,\n",
    "                    'startblock': start_block,\n",
    "                    'endblock': 'latest',\n",
    "                    'page': 1,\n",
    "                    'offset': 1000,\n",
    "                    'sort': 'desc',\n",
    "                    'apikey': self.etherscan_api_key\n",
    "                }\n",
    "\n",
    "                response = requests.get(url, params=params, timeout=30)\n",
    "\n",
    "                if response.status_code == 429:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"Rate limit hit for {wallet_address}, waiting {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "\n",
    "                data = response.json()\n",
    "\n",
    "                if data['status'] == '1':\n",
    "                    return data['result']\n",
    "                elif data['message'] == 'NOTOK' and 'rate limit' in data.get('result', '').lower():\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"Rate limit in response for {wallet_address}, waiting {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"API returned status 0 for {wallet_address}: {data.get('message', 'Unknown error')}\")\n",
    "                    return []\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "        print(f\"Failed to fetch transactions for {wallet_address} after {max_retries} attempts\")\n",
    "        return []\n",
    "\n",
    "    def parse_compound_transaction(self, tx):\n",
    "        \"\"\"\n",
    "        Parse a Compound transaction to extract action, amount, and asset information\n",
    "        \"\"\"\n",
    "        # This is a simplified parser - you'd need more sophisticated parsing\n",
    "        # for production use, potentially using contract ABIs\n",
    "        \n",
    "        compound_addresses = set(addr.lower() for addr in self.compound_contracts.values())\n",
    "        to_address = tx.get('to', '').lower()\n",
    "        \n",
    "        if to_address not in compound_addresses:\n",
    "            return None\n",
    "            \n",
    "        # Basic action classification based on method ID\n",
    "        input_data = tx.get('input', '')\n",
    "        value = float(tx.get('value', 0)) / 1e18\n",
    "        \n",
    "        action = 'unknown'\n",
    "        if input_data.startswith('0xa0712d68'):  # mint\n",
    "            action = 'supply'\n",
    "        elif input_data.startswith('0xdb006a75'):  # redeem\n",
    "            action = 'redeem'\n",
    "        elif input_data.startswith('0xc5ebeaec'):  # borrow\n",
    "            action = 'borrow'\n",
    "        elif input_data.startswith('0x0e752702'):  # repay\n",
    "            action = 'repay'\n",
    "        elif input_data.startswith('0xf5e3c462'):  # liquidate\n",
    "            action = 'liquidate'\n",
    "            \n",
    "        # Estimate USD value (simplified - would need price feeds for accuracy)\n",
    "        amount_usd = value * 2000  # Rough ETH price estimate\n",
    "        \n",
    "        return {\n",
    "            'userWallet': tx['from'],\n",
    "            'action': action,\n",
    "            'amount_usd': amount_usd,\n",
    "            'asset': 'ETH',  # Simplified\n",
    "            'market': to_address,\n",
    "            'timestamp_dt': datetime.fromtimestamp(int(tx['timeStamp'])),\n",
    "            'txHash': tx['hash']\n",
    "        }\n",
    "\n",
    "    def convert_transactions_to_dataframe(self, wallet_transactions):\n",
    "        \"\"\"\n",
    "        Convert transaction data to DataFrame format for feature engineering\n",
    "        \"\"\"\n",
    "        parsed_transactions = []\n",
    "        \n",
    "        for wallet, transactions in wallet_transactions.items():\n",
    "            for tx in transactions:\n",
    "                parsed_tx = self.parse_compound_transaction(tx)\n",
    "                if parsed_tx:\n",
    "                    parsed_transactions.append(parsed_tx)\n",
    "                    \n",
    "        return pd.DataFrame(parsed_transactions)\n",
    "\n",
    "    def calculate_data_driven_weights(self, features_df):\n",
    "        \"\"\"\n",
    "        Calculate scoring weights based on actual data correlations with fallback defaults\n",
    "        \"\"\"\n",
    "        # Default weights as fallback\n",
    "        default_weights = {\n",
    "            'liquidity_risk': 0.30,\n",
    "            'activity_risk': 0.25,\n",
    "            'volatility_risk': 0.20,\n",
    "            'diversification_risk': 0.15,\n",
    "            'behavioral_risk': 0.10\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            feature_importance = self.analyzer.justify_feature_weights(features_df)\n",
    "            \n",
    "            # Check if we have valid feature importance scores\n",
    "            if not feature_importance or all(np.isnan(list(feature_importance.values()))):\n",
    "                print(\"Warning: No valid feature importance scores, using default weights\")\n",
    "                self.weights = default_weights\n",
    "                return default_weights\n",
    "            \n",
    "            # Group features into risk categories\n",
    "            feature_categories = {\n",
    "                'liquidity_risk': ['liquidate_ratio', 'leverage_indicator', 'borrow_ratio'],\n",
    "                'activity_risk': ['days_since_last_activity', 'recency_score', 'transactions_per_day'],\n",
    "                'volatility_risk': ['transaction_volatility', 'transaction_size_consistency'],\n",
    "                'diversification_risk': ['portfolio_diversity', 'unique_assets', 'unique_markets'],\n",
    "                'behavioral_risk': ['repayment_discipline', 'supply_ratio', 'repay_ratio']\n",
    "            }\n",
    "            \n",
    "            category_weights = {}\n",
    "            \n",
    "            for category, features in feature_categories.items():\n",
    "                category_importance = 0\n",
    "                feature_count = 0\n",
    "                \n",
    "                for feature in features:\n",
    "                    if feature in feature_importance and not np.isnan(feature_importance[feature]):\n",
    "                        category_importance += feature_importance[feature]\n",
    "                        feature_count += 1\n",
    "                \n",
    "                if feature_count > 0:\n",
    "                    category_weights[category] = category_importance / feature_count\n",
    "                else:\n",
    "                    category_weights[category] = default_weights[category]\n",
    "            \n",
    "            # Check for NaN values and replace with defaults\n",
    "            for category in category_weights:\n",
    "                if np.isnan(category_weights[category]):\n",
    "                    category_weights[category] = default_weights[category]\n",
    "            \n",
    "            # Normalize category weights to sum to 1\n",
    "            total_weight = sum(category_weights.values())\n",
    "            if total_weight > 0 and not np.isnan(total_weight):\n",
    "                for category in category_weights:\n",
    "                    category_weights[category] /= total_weight\n",
    "            else:\n",
    "                print(\"Warning: Invalid total weight, using default weights\")\n",
    "                category_weights = default_weights\n",
    "            \n",
    "            self.weights = category_weights\n",
    "            return category_weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating data-driven weights: {str(e)}\")\n",
    "            print(\"Using default weights\")\n",
    "            self.weights = default_weights\n",
    "            return default_weights\n",
    "\n",
    "    def calculate_risk_score(self, wallet_features, weights):\n",
    "        \"\"\"\n",
    "        Calculate risk score using data-driven features and weights with NaN handling\n",
    "        \"\"\"\n",
    "        # Helper function to safely get feature value\n",
    "        def safe_get(feature_name, default=0):\n",
    "            value = wallet_features.get(feature_name, default)\n",
    "            return default if (np.isnan(value) or np.isinf(value)) else value\n",
    "        \n",
    "        # Risk components based on feature categories\n",
    "        risk_components = {}\n",
    "        \n",
    "        # Liquidity Risk\n",
    "        liquidate_ratio = safe_get('liquidate_ratio', 0)\n",
    "        leverage_indicator = safe_get('leverage_indicator', 0)\n",
    "        borrow_ratio = safe_get('borrow_ratio', 0)\n",
    "        \n",
    "        liquidity_risk = (\n",
    "            liquidate_ratio * 0.4 +\n",
    "            min(leverage_indicator, 2.0) * 0.4 +  # Cap leverage at 2.0\n",
    "            borrow_ratio * 0.2\n",
    "        )\n",
    "        risk_components['liquidity_risk'] = min(liquidity_risk, 1.0)\n",
    "        \n",
    "        # Activity Risk\n",
    "        days_inactive = safe_get('days_since_last_activity', 0)\n",
    "        recency_score = safe_get('recency_score', 0)\n",
    "        tx_per_day = safe_get('transactions_per_day', 0)\n",
    "        \n",
    "        activity_risk = (\n",
    "            min(days_inactive / 365, 1.0) * 0.5 +\n",
    "            (1 - recency_score) * 0.3 +\n",
    "            min(abs(tx_per_day - 0.1) * 10, 1.0) * 0.2  # Optimal ~0.1 tx/day\n",
    "        )\n",
    "        risk_components['activity_risk'] = min(activity_risk, 1.0)\n",
    "        \n",
    "        # Volatility Risk\n",
    "        tx_volatility = safe_get('transaction_volatility', 0)\n",
    "        size_consistency = safe_get('transaction_size_consistency', 1)\n",
    "        \n",
    "        volatility_risk = (\n",
    "            min(tx_volatility / 10000, 1.0) * 0.7 +  # Normalize volatility\n",
    "            (1 - min(size_consistency, 1.0)) * 0.3\n",
    "        )\n",
    "        risk_components['volatility_risk'] = min(volatility_risk, 1.0)\n",
    "        \n",
    "        # Diversification Risk\n",
    "        portfolio_diversity = safe_get('portfolio_diversity', 0)\n",
    "        unique_assets = safe_get('unique_assets', 1)\n",
    "        unique_markets = safe_get('unique_markets', 1)\n",
    "        \n",
    "        diversification_risk = (\n",
    "            (1 - min(portfolio_diversity, 1.0)) * 0.4 +\n",
    "            (1 - min(unique_assets / 5, 1.0)) * 0.3 +\n",
    "            (1 - min(unique_markets / 5, 1.0)) * 0.3\n",
    "        )\n",
    "        risk_components['diversification_risk'] = min(diversification_risk, 1.0)\n",
    "        \n",
    "        # Behavioral Risk\n",
    "        repayment_discipline = safe_get('repayment_discipline', 0)\n",
    "        supply_ratio = safe_get('supply_ratio', 0)\n",
    "        repay_ratio = safe_get('repay_ratio', 0)\n",
    "        \n",
    "        behavioral_risk = (\n",
    "            (1 - min(repayment_discipline, 1.0)) * 0.5 +\n",
    "            abs(supply_ratio - 0.5) * 0.3 +  # Optimal supply ratio ~0.5\n",
    "            abs(repay_ratio - 0.3) * 0.2   # Optimal repay ratio ~0.3\n",
    "        )\n",
    "        risk_components['behavioral_risk'] = min(behavioral_risk, 1.0)\n",
    "        \n",
    "        # Calculate weighted final score\n",
    "        final_score = 0\n",
    "        for component, risk_value in risk_components.items():\n",
    "            weight = weights.get(component, 0.2)  # Default weight if missing\n",
    "            if not (np.isnan(weight) or np.isnan(risk_value)):\n",
    "                final_score += weight * risk_value\n",
    "        \n",
    "        # Ensure final_score is valid\n",
    "        if np.isnan(final_score) or np.isinf(final_score):\n",
    "            final_score = 0.5  # Medium risk default\n",
    "        \n",
    "        # Convert to 0-1000 scale\n",
    "        return int(min(max(final_score * 1000, 0), 1000))\n",
    "\n",
    "    def score_wallets_enhanced(self, wallet_addresses, training_sample_size=100):\n",
    "        \"\"\"\n",
    "        Enhanced wallet scoring with data-driven feature selection and robust error handling\n",
    "        \"\"\"\n",
    "        print(f\"Starting enhanced scoring for {len(wallet_addresses)} wallets...\")\n",
    "        \n",
    "        # Step 1: Collect training data from sample of wallets\n",
    "        print(\"Step 1: Collecting training data...\")\n",
    "        training_wallets = wallet_addresses[:min(training_sample_size, len(wallet_addresses))]\n",
    "        training_transactions = {}\n",
    "        \n",
    "        for i, wallet in enumerate(training_wallets):\n",
    "            print(f\"Fetching training data {i+1}/{len(training_wallets)}: {wallet}\")\n",
    "            transactions = self.get_transactions(wallet)\n",
    "            if transactions:  # Only add if we got transactions\n",
    "                training_transactions[wallet] = transactions\n",
    "            time.sleep(0.3)  # Rate limiting\n",
    "            \n",
    "        # Step 2: Convert to DataFrame and engineer features\n",
    "        print(\"Step 2: Engineering features...\")\n",
    "        training_df = self.convert_transactions_to_dataframe(training_transactions)\n",
    "        \n",
    "        if training_df.empty or len(training_df) < 10:\n",
    "            print(\"Warning: Insufficient training data, using default weights\")\n",
    "            weights = {\n",
    "                'liquidity_risk': 0.30,\n",
    "                'activity_risk': 0.25,\n",
    "                'volatility_risk': 0.20,\n",
    "                'diversification_risk': 0.15,\n",
    "                'behavioral_risk': 0.10\n",
    "            }\n",
    "        else:\n",
    "            try:\n",
    "                features_df = self.analyzer.engineer_compound_features(training_df)\n",
    "                \n",
    "                # Step 3: Analyze features and calculate weights\n",
    "                print(\"Step 3: Analyzing features...\")\n",
    "                \n",
    "                # Only create visualizations if we have enough data\n",
    "                if len(features_df) >= 5:\n",
    "                    try:\n",
    "                        correlation_matrix = self.analyzer.create_correlation_heatmap(features_df)\n",
    "                        feature_importance = self.analyzer.justify_feature_weights(features_df)\n",
    "                        if feature_importance:\n",
    "                            self.analyzer.create_feature_importance_plot(feature_importance)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not create visualizations: {str(e)}\")\n",
    "                \n",
    "                # Step 4: Calculate data-driven weights\n",
    "                print(\"Step 4: Calculating data-driven weights...\")\n",
    "                weights = self.calculate_data_driven_weights(features_df)\n",
    "                \n",
    "                print(\"Data-driven weights:\")\n",
    "                for category, weight in weights.items():\n",
    "                    print(f\"  {category}: {weight:.3f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in feature analysis: {str(e)}\")\n",
    "                print(\"Using default weights\")\n",
    "                weights = {\n",
    "                    'liquidity_risk': 0.30,\n",
    "                    'activity_risk': 0.25,\n",
    "                    'volatility_risk': 0.20,\n",
    "                    'diversification_risk': 0.15,\n",
    "                    'behavioral_risk': 0.10\n",
    "                }\n",
    "        \n",
    "        # Step 5: Score all wallets\n",
    "        print(\"Step 5: Scoring all wallets...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, wallet in enumerate(wallet_addresses):\n",
    "            try:\n",
    "                print(f\"Scoring wallet {i+1}/{len(wallet_addresses)}: {wallet}\")\n",
    "                \n",
    "                # Get wallet transactions\n",
    "                transactions = self.get_transactions(wallet)\n",
    "                wallet_df = self.convert_transactions_to_dataframe({wallet: transactions})\n",
    "                \n",
    "                if wallet_df.empty:\n",
    "                    # High risk for wallets with no Compound activity\n",
    "                    score = 850\n",
    "                else:\n",
    "                    # Engineer features for this wallet\n",
    "                    wallet_features_df = self.analyzer.engineer_compound_features(wallet_df)\n",
    "                    \n",
    "                    if len(wallet_features_df) > 0:\n",
    "                        wallet_features = wallet_features_df.iloc[0].to_dict()\n",
    "                        score = self.calculate_risk_score(wallet_features, weights)\n",
    "                    else:\n",
    "                        score = 800\n",
    "                \n",
    "                results.append({\n",
    "                    'wallet_id': wallet,\n",
    "                    'score': score\n",
    "                })\n",
    "                \n",
    "                # Save progress\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    temp_df = pd.DataFrame(results)\n",
    "                    temp_df.to_csv('enhanced_wallet_scores.csv', index=False)\n",
    "                    print(f\"Progress saved - {i+1}/{len(wallet_addresses)} complete\")\n",
    "                \n",
    "                time.sleep(0.3)  # Rate limiting\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scoring wallet {wallet}: {str(e)}\")\n",
    "                results.append({\n",
    "                    'wallet_id': wallet,\n",
    "                    'score': 500  # Default medium risk\n",
    "                })\n",
    "        \n",
    "        # Save final results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv('enhanced_wallet_scores.csv', index=False)\n",
    "        print(f\"Enhanced scoring complete! Results saved to enhanced_wallet_scores.csv\")\n",
    "        \n",
    "        return results_df, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bd8ad",
   "metadata": {},
   "source": [
    "# Additional utility functions for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4590ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_existing_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Analyze existing transaction data if you have it\n",
    "    \"\"\"\n",
    "    print(\"Analyzing existing transaction data...\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    analyzer = CompoundFeatureAnalyzer()\n",
    "    \n",
    "    # Engineer features\n",
    "    features_df = analyzer.engineer_compound_features(df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    correlation_matrix = analyzer.create_correlation_heatmap(features_df)\n",
    "    feature_importance = analyzer.justify_feature_weights(features_df)\n",
    "    analyzer.create_feature_importance_plot(feature_importance)\n",
    "    \n",
    "    return features_df, feature_importance\n",
    "\n",
    "def create_risk_report(results_df, weights):\n",
    "    \"\"\"\n",
    "    Create a comprehensive risk analysis report\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "# Compound Protocol Risk Scoring Report\n",
    "\n",
    "## Summary Statistics\n",
    "- Total wallets analyzed: {len(results_df)}\n",
    "- Average risk score: {results_df['score'].mean():.1f}\n",
    "- High risk wallets (>750): {len(results_df[results_df['score'] > 750])}\n",
    "- Medium risk wallets (400-750): {len(results_df[(results_df['score'] >= 400) & (results_df['score'] <= 750)])}\n",
    "- Low risk wallets (<400): {len(results_df[results_df['score'] < 400])}\n",
    "\n",
    "## Data-Driven Weight Configuration\n",
    "\"\"\"\n",
    "    \n",
    "    for category, weight in weights.items():\n",
    "        report += f\"- {category.replace('_', ' ').title()}: {weight:.1%}\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "## Risk Distribution\n",
    "- 99th percentile: {results_df['score'].quantile(0.99):.0f}\n",
    "- 95th percentile: {results_df['score'].quantile(0.95):.0f}\n",
    "- 75th percentile: {results_df['score'].quantile(0.75):.0f}\n",
    "- 25th percentile: {results_df['score'].quantile(0.25):.0f}\n",
    "\n",
    "## Methodology\n",
    "This scoring system uses data-driven feature selection based on correlation analysis\n",
    "with actual risk indicators rather than arbitrary weights. Features are selected\n",
    "based on their statistical significance and correlation with liquidation events\n",
    "and other risk behaviors.\n",
    "\"\"\"\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ca004",
   "metadata": {},
   "source": [
    "# Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Compound Protocol Wallet Risk Scoring System ===\n",
      "\n",
      "Loaded 103 wallet addresses from 'Wallet id - Sheet1.csv' column 'wallet_id'\n",
      "Starting enhanced scoring for 103 wallets...\n",
      "Step 1: Collecting training data...\n",
      "Fetching training data 1/100: 0x0039f22efb07a647557c7c5d17854cfd6d489ef3\n",
      "Fetching training data 2/100: 0x06b51c6882b27cb05e712185531c1f74996dd988\n",
      "Fetching training data 2/100: 0x06b51c6882b27cb05e712185531c1f74996dd988\n",
      "Fetching training data 3/100: 0x0795732aacc448030ef374374eaae57d2965c16c\n",
      "Fetching training data 3/100: 0x0795732aacc448030ef374374eaae57d2965c16c\n",
      "Fetching training data 4/100: 0x0aaa79f1a86bc8136cd0d1ca0d51964f4e3766f9\n",
      "Fetching training data 4/100: 0x0aaa79f1a86bc8136cd0d1ca0d51964f4e3766f9\n",
      "Fetching training data 5/100: 0x0fe383e5abc200055a7f391f94a5f5d1f844b9ae\n",
      "Fetching training data 5/100: 0x0fe383e5abc200055a7f391f94a5f5d1f844b9ae\n",
      "Fetching training data 6/100: 0x104ae61d8d487ad689969a17807ddc338b445416\n",
      "Fetching training data 6/100: 0x104ae61d8d487ad689969a17807ddc338b445416\n",
      "Fetching training data 7/100: 0x111c7208a7e2af345d36b6d4aace8740d61a3078\n",
      "Fetching training data 7/100: 0x111c7208a7e2af345d36b6d4aace8740d61a3078\n",
      "Fetching training data 8/100: 0x124853fecb522c57d9bd5c21231058696ca6d596\n",
      "Fetching training data 8/100: 0x124853fecb522c57d9bd5c21231058696ca6d596\n",
      "Fetching training data 9/100: 0x13b1c8b0e696aff8b4fee742119b549b605f3cbc\n",
      "Fetching training data 9/100: 0x13b1c8b0e696aff8b4fee742119b549b605f3cbc\n",
      "Fetching training data 10/100: 0x1656f1886c5ab634ac19568cd571bc72f385fdf7\n",
      "Fetching training data 10/100: 0x1656f1886c5ab634ac19568cd571bc72f385fdf7\n",
      "Fetching training data 11/100: 0x1724e16cb8d0e2aa4d08035bc6b5c56b680a3b22\n",
      "Fetching training data 11/100: 0x1724e16cb8d0e2aa4d08035bc6b5c56b680a3b22\n",
      "Fetching training data 12/100: 0x19df3e87f73c4aaf4809295561465b993e102668\n",
      "Fetching training data 12/100: 0x19df3e87f73c4aaf4809295561465b993e102668\n",
      "Fetching training data 13/100: 0x1ab2ccad4fc97c9968ea87d4435326715be32872\n",
      "Fetching training data 13/100: 0x1ab2ccad4fc97c9968ea87d4435326715be32872\n",
      "Fetching training data 14/100: 0x1c1b30ca93ef57452d53885d97a74f61daf2bf4f\n",
      "Fetching training data 14/100: 0x1c1b30ca93ef57452d53885d97a74f61daf2bf4f\n",
      "Fetching training data 15/100: 0x1e43dacdcf863676a6bec8f7d6896d6252fac669\n",
      "Fetching training data 15/100: 0x1e43dacdcf863676a6bec8f7d6896d6252fac669\n",
      "Fetching training data 16/100: 0x22d7510588d90ed5a87e0f838391aaafa707c34b\n",
      "Fetching training data 16/100: 0x22d7510588d90ed5a87e0f838391aaafa707c34b\n",
      "Fetching training data 17/100: 0x24b3460622d835c56d9a4fe352966b9bdc6c20af\n",
      "Fetching training data 17/100: 0x24b3460622d835c56d9a4fe352966b9bdc6c20af\n",
      "Fetching training data 18/100: 0x26750f1f4277221bdb5f6991473c6ece8c821f9d\n",
      "Fetching training data 18/100: 0x26750f1f4277221bdb5f6991473c6ece8c821f9d\n",
      "Fetching training data 19/100: 0x27f72a000d8e9f324583f3a3491ea66998275b28\n",
      "Fetching training data 19/100: 0x27f72a000d8e9f324583f3a3491ea66998275b28\n",
      "Fetching training data 20/100: 0x2844658bf341db96aa247259824f42025e3bcec2\n",
      "Fetching training data 20/100: 0x2844658bf341db96aa247259824f42025e3bcec2\n",
      "Fetching training data 21/100: 0x2a2fde3e1beb508fcf7c137a1d5965f13a17825e\n",
      "Fetching training data 21/100: 0x2a2fde3e1beb508fcf7c137a1d5965f13a17825e\n",
      "Fetching training data 22/100: 0x330513970efd9e8dd606275fb4c50378989b3204\n",
      "Fetching training data 22/100: 0x330513970efd9e8dd606275fb4c50378989b3204\n",
      "Fetching training data 23/100: 0x3361bea43c2f5f963f81ac70f64e6fba1f1d2a97\n",
      "Fetching training data 23/100: 0x3361bea43c2f5f963f81ac70f64e6fba1f1d2a97\n",
      "Fetching training data 24/100: 0x3867d222ba91236ad4d12c31056626f9e798629c\n",
      "Fetching training data 24/100: 0x3867d222ba91236ad4d12c31056626f9e798629c\n",
      "Fetching training data 25/100: 0x3a44be4581137019f83021eeee72b7dc57756069\n",
      "Fetching training data 25/100: 0x3a44be4581137019f83021eeee72b7dc57756069\n",
      "Fetching training data 26/100: 0x3e69ad05716bdc834db72c4d6d44439a7c8a902b\n",
      "Fetching training data 26/100: 0x3e69ad05716bdc834db72c4d6d44439a7c8a902b\n",
      "Fetching training data 27/100: 0x427f2ac5fdf4245e027d767e7c3ac272a1f40a65\n",
      "Fetching training data 27/100: 0x427f2ac5fdf4245e027d767e7c3ac272a1f40a65\n",
      "Fetching training data 28/100: 0x4814be124d7fe3b240eb46061f7ddfab468fe122\n",
      "Fetching training data 28/100: 0x4814be124d7fe3b240eb46061f7ddfab468fe122\n",
      "Fetching training data 29/100: 0x4839e666e2baf12a51bf004392b35972eeddeabf\n",
      "Fetching training data 29/100: 0x4839e666e2baf12a51bf004392b35972eeddeabf\n",
      "Fetching training data 30/100: 0x4c4d05fe859279c91b074429b5fc451182cec745\n",
      "Fetching training data 30/100: 0x4c4d05fe859279c91b074429b5fc451182cec745\n",
      "Fetching training data 31/100: 0x4d997c89bc659a3e8452038a8101161e7e7e53a7\n",
      "Fetching training data 31/100: 0x4d997c89bc659a3e8452038a8101161e7e7e53a7\n",
      "Fetching training data 32/100: 0x4db0a72edb5ea6c55df929f76e7d5bb14e389860\n",
      "Fetching training data 32/100: 0x4db0a72edb5ea6c55df929f76e7d5bb14e389860\n",
      "Fetching training data 33/100: 0x4e61251336c32e4fe6bfd5fab014846599321389\n",
      "Fetching training data 33/100: 0x4e61251336c32e4fe6bfd5fab014846599321389\n",
      "Fetching training data 34/100: 0x4e6e724f4163b24ffc7ffe662b5f6815b18b4210\n",
      "Fetching training data 34/100: 0x4e6e724f4163b24ffc7ffe662b5f6815b18b4210\n",
      "Fetching training data 35/100: 0x507b6c0d950702f066a9a1bd5e85206f87b065ba\n",
      "Fetching training data 35/100: 0x507b6c0d950702f066a9a1bd5e85206f87b065ba\n",
      "Fetching training data 36/100: 0x54e19653be9d4143b08994906be0e27555e8834d\n",
      "Fetching training data 36/100: 0x54e19653be9d4143b08994906be0e27555e8834d\n",
      "Fetching training data 37/100: 0x56ba823641bfc317afc8459bf27feed6eb9ff59f\n",
      "Fetching training data 37/100: 0x56ba823641bfc317afc8459bf27feed6eb9ff59f\n",
      "Fetching training data 38/100: 0x56cc2bffcb3f86a30c492f9d1a671a1f744d1d2f\n",
      "Fetching training data 38/100: 0x56cc2bffcb3f86a30c492f9d1a671a1f744d1d2f\n",
      "Fetching training data 39/100: 0x578cea5f899b0dfbf05c7fbcfda1a644b2a47787\n",
      "Fetching training data 39/100: 0x578cea5f899b0dfbf05c7fbcfda1a644b2a47787\n",
      "Fetching training data 40/100: 0x58c2a9099a03750e9842d3e9a7780cdd6aa70b86\n",
      "Fetching training data 40/100: 0x58c2a9099a03750e9842d3e9a7780cdd6aa70b86\n",
      "Fetching training data 41/100: 0x58d68d4bcf9725e40353379cec92b90332561683\n",
      "Fetching training data 41/100: 0x58d68d4bcf9725e40353379cec92b90332561683\n",
      "Fetching training data 42/100: 0x5e324b4a564512ea7c93088dba2f8c1bf046a3eb\n",
      "Fetching training data 42/100: 0x5e324b4a564512ea7c93088dba2f8c1bf046a3eb\n",
      "Fetching training data 43/100: 0x612a3500559be7be7703de6dc397afb541a16f7f\n",
      "Fetching training data 43/100: 0x612a3500559be7be7703de6dc397afb541a16f7f\n",
      "Fetching training data 44/100: 0x623af911f493747c216ad389c7805a37019c662d\n",
      "Fetching training data 44/100: 0x623af911f493747c216ad389c7805a37019c662d\n",
      "Fetching training data 45/100: 0x6a2752a534faacaaa153bffbb973dd84e0e5497b\n",
      "Fetching training data 45/100: 0x6a2752a534faacaaa153bffbb973dd84e0e5497b\n",
      "Fetching training data 46/100: 0x6d69ca3711e504658977367e13c300ab198379f1\n",
      "Fetching training data 46/100: 0x6d69ca3711e504658977367e13c300ab198379f1\n",
      "Fetching training data 47/100: 0x6e355417f7f56e7927d1cd971f0b5a1e6d538487\n",
      "Fetching training data 47/100: 0x6e355417f7f56e7927d1cd971f0b5a1e6d538487\n",
      "Fetching training data 48/100: 0x70c1864282599a762c674dd9d567b37e13bce755\n",
      "Fetching training data 48/100: 0x70c1864282599a762c674dd9d567b37e13bce755\n",
      "Fetching training data 49/100: 0x70d8e4ab175dfe0eab4e9a7f33e0a2d19f44001e\n",
      "Fetching training data 49/100: 0x70d8e4ab175dfe0eab4e9a7f33e0a2d19f44001e\n",
      "Fetching training data 50/100: 0x7399dbeebe2f88bc6ac4e3fd7ddb836a4bce322f\n",
      "Fetching training data 50/100: 0x7399dbeebe2f88bc6ac4e3fd7ddb836a4bce322f\n",
      "Fetching training data 51/100: 0x767055590c73b7d2aaa6219da13807c493f91a20\n",
      "Fetching training data 51/100: 0x767055590c73b7d2aaa6219da13807c493f91a20\n",
      "Fetching training data 52/100: 0x7851bdfb64bbecfb40c030d722a1f147dff5db6a\n",
      "Fetching training data 52/100: 0x7851bdfb64bbecfb40c030d722a1f147dff5db6a\n",
      "Fetching training data 53/100: 0x7b4636320daa0bc055368a4f9b9d01bd8ac51877\n",
      "Fetching training data 53/100: 0x7b4636320daa0bc055368a4f9b9d01bd8ac51877\n",
      "Fetching training data 54/100: 0x7b57dbe2f2e4912a29754ff3e412ed9507fd8957\n",
      "Fetching training data 54/100: 0x7b57dbe2f2e4912a29754ff3e412ed9507fd8957\n",
      "Fetching training data 55/100: 0x7be3dfb5b6fcbae542ea85e76cc19916a20f6c1e\n",
      "Fetching training data 55/100: 0x7be3dfb5b6fcbae542ea85e76cc19916a20f6c1e\n",
      "Fetching training data 56/100: 0x7de76a449cf60ea3e111ff18b28e516d89532152\n",
      "Fetching training data 56/100: 0x7de76a449cf60ea3e111ff18b28e516d89532152\n",
      "Fetching training data 57/100: 0x7e3eab408b9c76a13305ef34606f17c16f7b33cc\n",
      "Fetching training data 57/100: 0x7e3eab408b9c76a13305ef34606f17c16f7b33cc\n",
      "Fetching training data 58/100: 0x7f5e6a28afc9fb0aaf4259d4ff69991b88ebea47\n",
      "Fetching training data 58/100: 0x7f5e6a28afc9fb0aaf4259d4ff69991b88ebea47\n",
      "Fetching training data 59/100: 0x83ea74c67d393c6894c34c464657bda2183a2f1a\n",
      "Fetching training data 59/100: 0x83ea74c67d393c6894c34c464657bda2183a2f1a\n",
      "Fetching training data 60/100: 0x8441fecef5cc6f697be2c4fc4a36feacede8df67\n",
      "Fetching training data 60/100: 0x8441fecef5cc6f697be2c4fc4a36feacede8df67\n",
      "Fetching training data 61/100: 0x854a873b8f9bfac36a5eb9c648e285a095a7478d\n",
      "Fetching training data 61/100: 0x854a873b8f9bfac36a5eb9c648e285a095a7478d\n",
      "Fetching training data 62/100: 0x8587d9f794f06d976c2ec1cfd523983b856f5ca9\n",
      "Fetching training data 62/100: 0x8587d9f794f06d976c2ec1cfd523983b856f5ca9\n",
      "Fetching training data 63/100: 0x880a0af12da55df1197f41697c1a1b61670ed410\n",
      "Fetching training data 63/100: 0x880a0af12da55df1197f41697c1a1b61670ed410\n",
      "Fetching training data 64/100: 0x8aaece100580b749a20f8ce30338c4e0770b65ed\n",
      "Fetching training data 64/100: 0x8aaece100580b749a20f8ce30338c4e0770b65ed\n",
      "Fetching training data 65/100: 0x8be38ea2b22b706aef313c2de81f7d179024dd30\n",
      "Fetching training data 65/100: 0x8be38ea2b22b706aef313c2de81f7d179024dd30\n",
      "Fetching training data 66/100: 0x8d900f213db5205c529aaba5d10e71a0ed2646db\n",
      "Fetching training data 66/100: 0x8d900f213db5205c529aaba5d10e71a0ed2646db\n",
      "Fetching training data 67/100: 0x91919344c1dad09772d19ad8ad4f1bcd29c51f27\n",
      "Fetching training data 67/100: 0x91919344c1dad09772d19ad8ad4f1bcd29c51f27\n",
      "Fetching training data 68/100: 0x93f0891bf71d8abed78e0de0885bd26355bb8b1d\n",
      "Fetching training data 68/100: 0x93f0891bf71d8abed78e0de0885bd26355bb8b1d\n",
      "Fetching training data 69/100: 0x96479b087cb8f236a5e2dcbfc50ce63b2f421da6\n",
      "Fetching training data 69/100: 0x96479b087cb8f236a5e2dcbfc50ce63b2f421da6\n",
      "Fetching training data 70/100: 0x96bb4447a02b95f1d1e85374cffd565eb22ed2f8\n",
      "Fetching training data 70/100: 0x96bb4447a02b95f1d1e85374cffd565eb22ed2f8\n",
      "Fetching training data 71/100: 0x9a363adc5d382c04d36b09158286328f75672098\n",
      "Fetching training data 71/100: 0x9a363adc5d382c04d36b09158286328f75672098\n",
      "Fetching training data 72/100: 0x9ad1331c5b6c5a641acffb32719c66a80c6e1a17\n",
      "Fetching training data 72/100: 0x9ad1331c5b6c5a641acffb32719c66a80c6e1a17\n",
      "Fetching training data 73/100: 0x9ba0d85f71e145ccf15225e59631e5a883d5d74a\n",
      "Fetching training data 73/100: 0x9ba0d85f71e145ccf15225e59631e5a883d5d74a\n",
      "Fetching training data 74/100: 0x9e6ec4e98793970a1307262ba68d37594e58cd78\n",
      "Fetching training data 74/100: 0x9e6ec4e98793970a1307262ba68d37594e58cd78\n",
      "Fetching training data 75/100: 0xa7e94d933eb0c439dda357f61244a485246e97b8\n",
      "Fetching training data 75/100: 0xa7e94d933eb0c439dda357f61244a485246e97b8\n",
      "Fetching training data 76/100: 0xa7f3c74f0255796fd5d3ddcf88db769f7a6bf46a\n",
      "Fetching training data 76/100: 0xa7f3c74f0255796fd5d3ddcf88db769f7a6bf46a\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"=== Enhanced Compound Protocol Wallet Risk Scoring System ===\")\n",
    "    print()\n",
    "    \n",
    "    # API Configuration\n",
    "    ETHERSCAN_API_KEY = \"2E8Z182IFISX8QNJHGQP7RD9S8CMUYGVXQ\"\n",
    "    INFURA_URL = \"https://mainnet.infura.io/v3/1daa5e5385054a21932c3bc33dff2d6b\"\n",
    "    \n",
    "    # Initialize scorer\n",
    "    scorer = CompoundRiskScorer(\n",
    "        infura_url=INFURA_URL,\n",
    "        etherscan_api_key=ETHERSCAN_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Only use 'Wallet id - Sheet1.csv' for wallet addresses\n",
    "    csv_filename = \"Wallet id - Sheet1.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        wallet_column = None\n",
    "        possible_columns = ['wallet_address', 'address', 'wallet', 'wallet_id', 'account', 'userWallet', 'user_wallet']\n",
    "        for col in possible_columns:\n",
    "            if col in df.columns:\n",
    "                wallet_column = col\n",
    "                break\n",
    "        if wallet_column is None:\n",
    "            print(\"Could not automatically detect wallet address column.\")\n",
    "            print(\"Available columns:\", list(df.columns))\n",
    "            wallet_column = input(\"Please enter the column name containing wallet addresses: \")\n",
    "        wallet_addresses = df[wallet_column].dropna().unique().tolist()\n",
    "        print(f\"Loaded {len(wallet_addresses)} wallet addresses from '{csv_filename}' column '{wallet_column}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading CSV file: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Run enhanced scoring\n",
    "    results_df, weights = scorer.score_wallets_enhanced(wallet_addresses)\n",
    "\n",
    "    print(\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Scored {len(results_df)} wallets\")\n",
    "    print(f\"\\nScore distribution:\")\n",
    "    print(f\"Mean: {results_df['score'].mean():.1f}\")\n",
    "    print(f\"Median: {results_df['score'].median():.1f}\")\n",
    "    print(f\"Min: {results_df['score'].min()}\")\n",
    "    print(f\"Max: {results_df['score'].max()}\")\n",
    "\n",
    "    print(f\"\\nData-driven category weights used:\")\n",
    "    for category, weight in weights.items():\n",
    "        print(f\"  {category}: {weight:.1%}\")\n",
    "\n",
    "    # Save the ML model (scaler and weights) in the current folder\n",
    "    import joblib\n",
    "    model_artifacts = {\n",
    "        'scaler': scorer.scaler,\n",
    "        'weights': weights\n",
    "    }\n",
    "    joblib.dump(model_artifacts, 'compound_risk_model.joblib')\n",
    "    print(\"\\nML model artifacts saved to 'compound_risk_model.joblib'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca9c8e",
   "metadata": {},
   "source": [
    "# Results and Interpretation\n",
    "\n",
    "The risk scoring process is now complete. The following outputs have been generated:\n",
    "- **enhanced_wallet_scores.csv**: Contains the risk scores for each wallet address.\n",
    "- **compound_risk_model.joblib**: Stores the model scaler and weights for reproducibility or further analysis.\n",
    "\n",
    "## How to Interpret the Results\n",
    "- **0-399**: Low risk wallets\n",
    "- **400-749**: Medium risk wallets\n",
    "- **750-1000**: High risk wallets\n",
    "\n",
    "You can use these scores to identify potentially risky wallets, analyze behavioral patterns, or further refine your risk assessment methodology.\n",
    "\n",
    "For a detailed explanation of the methodology, feature selection, and risk indicator justification, refer to the `approach.md` file.\n",
    "\n",
    "---\n",
    "**Next steps:**\n",
    "- Review the generated CSV and model artifacts.\n",
    "- Use the results for reporting, further analysis, or integration into downstream systems.\n",
    "- Adjust feature engineering or scoring logic as needed for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a634e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
